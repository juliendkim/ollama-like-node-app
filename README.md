# Ollama-like Node.js App

![Screenshot](screenshot.png)

<details>
<summary><strong>ğŸ‡°ğŸ‡· Korean (í•œêµ­ì–´) - Click to expand</strong></summary>

# Ollama-like Node.js App

ì´ í”„ë¡œì íŠ¸ëŠ” ë¡œì»¬ í™˜ê²½ì—ì„œ LLM(Large Language Model)ì„ ì‹¤í–‰í•˜ê³  ì±„íŒ…í•  ìˆ˜ ìˆëŠ” Node.js ì• í”Œë¦¬ì¼€ì´ì…˜ì…ë‹ˆë‹¤. Ollamaì™€ ìœ ì‚¬í•˜ê²Œ ì‘ë™í•˜ë©°, ONNX Runtimeì„ í†µí•´ ê²½ëŸ‰í™”ëœ ëª¨ë¸ì„ íš¨ìœ¨ì ìœ¼ë¡œ êµ¬ë™í•©ë‹ˆë‹¤.

## ğŸš€ íŠ¹ì§•

- **ë¡œì»¬ ì„œë²„**: Express.js ê¸°ë°˜ì˜ REST API ì„œë²„ (`server.js`)
- **CLI ì±„íŒ… í´ë¼ì´ì–¸íŠ¸**: í„°ë¯¸ë„ì—ì„œ ë°”ë¡œ ëŒ€í™”í•  ìˆ˜ ìˆëŠ” í´ë¼ì´ì–¸íŠ¸ (`chat.js`)
- **ëª¨ë¸ ë‹¤ìš´ë¡œë”**: í•„ìš”í•œ ONNX ëª¨ë¸ì„ ìë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ (`download-model.js`)
- **ëŒ€í™” ê¸°ë¡**: ë©€í‹°í„´ ëŒ€í™”ë¥¼ ì§€ì›í•˜ì—¬ ì´ì „ ë¬¸ë§¥ì„ ê¸°ì–µí•©ë‹ˆë‹¤.

## ğŸ’¡ ONNXë¥¼ ì‚¬ìš©í•˜ëŠ” ì´ìœ 

ì´ í”„ë¡œì íŠ¸ëŠ” **ONNX (Open Neural Network Exchange)** í˜•ì‹ì˜ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ê·¸ ì´ìœ ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

1.  **íš¨ìœ¨ì ì¸ ë¡œì»¬ ì‹¤í–‰**: ONNX Runtimeì€ ë‹¤ì–‘í•œ í•˜ë“œì›¨ì–´(CPU, GPU ë“±)ì—ì„œ ìµœì í™”ëœ ì„±ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤. íŠ¹íˆ `q4`(4ë¹„íŠ¸ ì–‘ìí™”) ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ë©´ì„œë„ ì¤€ìˆ˜í•œ ì„±ëŠ¥ì„ ëƒ…ë‹ˆë‹¤.
2.  **í”„ë¼ì´ë²„ì‹œ**: ëª¨ë“  ë°ì´í„° ì²˜ë¦¬ê°€ ì‚¬ìš©ìì˜ ì»´í“¨í„° ë‚´ì—ì„œ ì´ë£¨ì–´ì§€ë¯€ë¡œ, ë¯¼ê°í•œ ë°ì´í„°ê°€ ì™¸ë¶€ ì„œë²„ë¡œ ì „ì†¡ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
3.  **ë¹„ìš© ì ˆê°**: ì™¸ë¶€ API(OpenAI, Anthropic ë“±)ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ í† í° ë¹„ìš©ì´ ë°œìƒí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
4.  **í˜¸í™˜ì„±**: PyTorchë‚˜ TensorFlowë¡œ í•™ìŠµëœ ëª¨ë¸ì„ í‘œì¤€ í¬ë§·ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ë‹¤ì–‘í•œ í™˜ê²½ì—ì„œ ì‰½ê²Œ ë°°í¬í•˜ê³  ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ğŸ› ï¸ ì„¤ì¹˜ ë° ì„¤ì • (Setup)

### 1. ì‚¬ì „ ìš”êµ¬ ì‚¬í•­
- Node.js (v18 ì´ìƒ ê¶Œì¥)
- npm

### 2. íŒ¨í‚¤ì§€ ì„¤ì¹˜
í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ì—ì„œ ì˜ì¡´ì„±ì„ ì„¤ì¹˜í•©ë‹ˆë‹¤.
```bash
npm install
```

### 3. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
ì„œë²„ ì‹¤í–‰ ì „, ì‚¬ìš©í•  ONNX ëª¨ë¸(`onnx-community/gemma-3-1b-it-ONNX`)ì„ ë‹¤ìš´ë¡œë“œí•´ì•¼ í•©ë‹ˆë‹¤.
```bash
npm run download
```
> ëª¨ë¸ì€ `./models` ë””ë ‰í† ë¦¬ì— ì €ì¥ë©ë‹ˆë‹¤.

## ğŸ–¥ï¸ ì‚¬ìš© ë°©ë²• (Usage)

### ì„œë²„ ì‹¤í–‰
ë¨¼ì € API ì„œë²„ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤. ì„œë²„ëŠ” 3000ë²ˆ í¬íŠ¸ì—ì„œ ìš”ì²­ì„ ëŒ€ê¸°í•©ë‹ˆë‹¤.
```bash
npm run serve
```
*ì„œë²„ê°€ ì‹œì‘ë˜ë©´ ëª¨ë¸ì„ ë¡œë”©í•˜ë©°, "Model loaded successfully!" ë©”ì‹œì§€ê°€ ëœ° ë•Œê¹Œì§€ ì ì‹œ ê¸°ë‹¤ë ¤ì•¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.*

### ì±„íŒ… ì‹œì‘
ìƒˆë¡œìš´ í„°ë¯¸ë„ ì°½ì„ ì—´ê³  CLI ì±„íŒ… í´ë¼ì´ì–¸íŠ¸ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
```bash
npm run chat
```

## ğŸ“‚ í”„ë¡œì íŠ¸ êµ¬ì¡°

- **`server.js`**: ë©”ì¸ ì„œë²„ íŒŒì¼ì…ë‹ˆë‹¤. Express ì•±ì„ ì„¤ì •í•˜ê³  `@huggingface/transformers`ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ë¡œë“œ ë° ì¶”ë¡ í•©ë‹ˆë‹¤.
- **`chat.js`**: ì‚¬ìš©ìì˜ ì…ë ¥ì„ ë°›ì•„ ì„œë²„ë¡œ ì „ì†¡í•˜ê³  ì‘ë‹µì„ ì¶œë ¥í•˜ëŠ” ê°„ë‹¨í•œ CLI ë„êµ¬ì…ë‹ˆë‹¤.
- **`download-model.js`**: ì§€ì •ëœ ONNX ëª¨ë¸ì„ ë¡œì»¬ì— ë‹¤ìš´ë¡œë“œí•˜ëŠ” ìœ í‹¸ë¦¬í‹° ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.
- **`models/`**: ë‹¤ìš´ë¡œë“œëœ ëª¨ë¸ íŒŒì¼ì´ ì €ì¥ë˜ëŠ” ìœ„ì¹˜ì…ë‹ˆë‹¤.

## ğŸ”Œ API ëª…ì„¸

### `POST /chat`
ì±„íŒ… ë©”ì‹œì§€ë¥¼ ë³´ë‚´ê³  ëª¨ë¸ì˜ ì‘ë‹µì„ ë°›ìŠµë‹ˆë‹¤.

**Request Body:**
```json
{
  "messages": [
    { "role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”?" }
  ]
}
```
ë˜ëŠ” ë‹¨ì¼ ë©”ì‹œì§€:
```json
{
  "message": "ì•ˆë…•í•˜ì„¸ìš”?"
}
```

**Response:**
```json
{
  "response": "ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"
}
```

</details>

---

This project is a Node.js application that allows you to run and chat with a Large Language Model (LLM) locally. It operates similarly to Ollama, efficiently driving lightweight models via ONNX Runtime.

## ğŸš€ Features

- **Local Server**: Express.js-based REST API server (`server.js`).
- **CLI Chat Client**: Terminal-based client for direct conversation (`chat.js`).
- **Model Downloader**: Script to automatically download the required ONNX model (`download-model.js`).
- **Conversation History**: Supports multi-turn conversations, remembering context.

## ğŸ’¡ Why ONNX?

This project uses models in the **ONNX (Open Neural Network Exchange)** format. Here is why:

1.  **Efficient Local Execution**: ONNX Runtime offers optimized performance across various hardware (CPU, GPU, etc.). Using `q4` (4-bit quantization) models significantly reduces memory usage while maintaining decent performance.
2.  **Privacy**: All data processing happens locally on your machine. No sensitive data is sent to external servers.
3.  **Cost Saving**: No API fees (like OpenAI or Anthropic) are incurred as it runs on your hardware.
4.  **Compatibility**: Models trained in PyTorch or TensorFlow can be converted to this standard format for easy deployment across different environments.

## ğŸ› ï¸ Setup

### 1. Prerequisites
- Node.js (v18 or higher recommended)
- npm

### 2. Install Dependencies
Install dependencies in the project root directory.
```bash
npm install
```

### 3. Download Model
Before running the server, you need to download the ONNX model (`onnx-community/gemma-3-1b-it-ONNX`).
```bash
npm run download
```
> The model will be saved in the `./models` directory.

## ğŸ–¥ï¸ Usage

### Run Server
Start the API server first. It listens on port 3000.
```bash
npm run serve
```
*Once the server starts, it will load the model. Please wait until you see the "Model loaded successfully!" message.*

### Start Chat
Open a new terminal window and run the CLI chat client.
```bash
npm run chat
```

## ğŸ“‚ Project Structure

- **`server.js`**: Main server file. Sets up the Express app and uses `@huggingface/transformers` to load and infer the model.
- **`chat.js`**: A simple CLI tool that takes user input, sends it to the server, and outputs the response.
- **`download-model.js`**: Utility script to download the specified ONNX model locally.
- **`models/`**: Directory where downloaded model files are stored.

## ğŸ”Œ API Specification

### `POST /chat`
Sends a chat message and receives the model's response.

**Request Body:**
```json
{
  "messages": [
    { "role": "user", "content": "Hello?" }
  ]
}
```
Or a single message:
```json
{
  "message": "Hello?"
}
```

**Response:**
```json
{
  "response": "Hello! How can I help you today?"
}
```

<br>

